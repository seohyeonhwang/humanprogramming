{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-cc5eac366280>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-cc5eac366280>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sudo apt-get install openjdk-8-jre\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sudo apt-get install openjdk-8-jre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-892f36910589>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-892f36910589>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    //file에=열어서 저장해라()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from konlpy.tag import Twitter\n",
    "from gensim.models import Word2Vec\n",
    "import csv //comma separated values, txt형 파일확장자\n",
    " \n",
    "twitter = Twitter()\n",
    " \n",
    "file = open(\"Article_shuffled.csv\", 'r', encoding='euc-kr')\n",
    "    //file에=열어서 저장해라(\"Article_shuffled를\")\n",
    "line = csv.reader(file)\n",
    "token = []\n",
    "embeddingmodel = []\n",
    " \n",
    "for i in line:\n",
    "    sentence = twitter.pos(i[0], norm=True, stem=True)\n",
    "        //sentence에 = twitter.내장함수인pos(part of speech=형태소)를(line 별로 저장)\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0])\n",
    "            //temp_embedding에는 index 정보 저장\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1])\n",
    "            //temp에는 index 정보/날짜정보 저장\n",
    "        //for 문을 돌면 temp_embedding에는 리스트별 index 정보가, temp에는 리스트별 index/날짜 정보가\n",
    "    all_temp.append(temp)\n",
    "        //all_temp에는 temp저장\n",
    "    embeddingmodel.append(temp_embedding)\n",
    "        //embeddingmodel에는 temp_embedding 저장\n",
    "    category_number_dic = {'IT과학': 0, '경제': 1, '정치': 2,'세계':3,'오피니언':4, '사회': 5, '생활문화': 6}\n",
    "    all_temp.append(category_number_dic.get(category))\n",
    "        //all_temp에 카테고리(it과학:0)정보까지 저장\n",
    "    token.append(all_temp)\n",
    "print(\"토큰 처리 완료\")\n",
    "    //여기까지 하면 all_temp에 index + 날짜 + 카테고리dic 정보 저장되어있음\n",
    " \n",
    "embeddingmodel = []\n",
    "for i in range(len(token)): ////index를 돌고\n",
    "    temp_embeddingmodel = []\n",
    "    for k in range(len(token[i][0])): ////날짜를 돌고\n",
    "        temp_embeddingmodel.append(token[i][0][k])\n",
    "    embeddingmodel.append(temp_embeddingmodel)\n",
    "        ////돌면서 temp_embeddingmodel에 넣고\n",
    "// max_vocab size 10000000 개당 1 GB 메모리 차지\n",
    "embedding = Word2Vec(embeddingmodel, size=300, window=5, min_count=10, iter=5, sg=1, max_vocab_size = 360000000)\n",
    "embedding.save('post.embedding') //만들어진 사각형 숫자 정보를 -> vector로 바꾸는 모델\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 21 14:30:52 2018\n",
    "\n",
    "@author: jbk48\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class Bi_LSTM():\n",
    "    \n",
    "    def __init__(self, lstm_units, num_class, keep_prob):\n",
    "        \n",
    "        self.lstm_units = lstm_units\n",
    "        \n",
    "        with tf.variable_scope('forward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('backward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "        \n",
    "        with tf.variable_scope('Weights', reuse = tf.AUTO_REUSE):\n",
    "           \n",
    "            self.W = tf.get_variable(name=\"W\", shape=[2 * lstm_units, num_class],\n",
    "                                dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
    "            self.b = tf.get_variable(name=\"b\", shape=[num_class], dtype=tf.float32,\n",
    "                                initializer=tf.zeros_initializer())\n",
    "            \n",
    "            \n",
    "    def logits(self, X, W, b, seq_len):\n",
    "        \n",
    "        (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, self.lstm_bw_cell,dtype=tf.float32,\n",
    "                                                                            inputs = X, sequence_length = seq_len)\n",
    "        ## concat fw, bw final states\n",
    "        outputs = tf.concat([states[0][1], states[1][1]], axis=1)\n",
    "        pred = tf.matmul(outputs, W) + b        \n",
    "        return pred\n",
    "        \n",
    "    def model_build(self, logits, labels, learning_rate = 0.001):\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits , labels = labels)) # Softmax loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # Adam Optimizer\n",
    "            \n",
    "        return loss, optimizer\n",
    "    \n",
    "    def graph_build(self, avg_loss, avg_acc):\n",
    "        \n",
    "        tf.summary.scalar('Loss', avg_loss)\n",
    "        tf.summary.scalar('Accuracy', avg_acc)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-ae642fb00228>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-ae642fb00228>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    for i in line: ///line으로 자르기\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: MsWon\n",
    "@editor: lumyjuwon\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Bi_LSTM as Bi_LSTM\n",
    "import Word2Vec as Word2Vec\n",
    "import csv\n",
    "from konlpy.tag import Twitter\n",
    "import os\n",
    "\n",
    "twitter = Twitter()\n",
    "W2V = Word2Vec.Word2Vec()\n",
    "\n",
    "file = open(\"Article_shuffled.csv\", 'r', encoding='euc-kr')\n",
    "line = csv.reader(file)\n",
    "token = []\n",
    "embeddingmodel = []\n",
    "\n",
    "for i in line: ///line으로 자르기\n",
    "    content = i[3]  # csv에서 뉴스 제목 또는 뉴스 본문 column으로 변경\n",
    "    sentence = twitter.pos(i[0], norm=True, stem=True) ///line안에 형태소별로 자르기\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0]) ///temp_embedding에는 (형태소들 저장)\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1]) /// temp에는 (형태소[0] / 해당품사[1] 저장)\n",
    "    all_temp.append(temp) \n",
    "    embeddingmodel.append(temp_embedding)\n",
    "    category = i[1]  # csv에서 category column으로 변경\n",
    "    ///for문 다 돌면 all_temp에는 모든 문장의 형태소[0]/품사[1]가 저장 , embeddingmodel에는 모든 문장의 품사[1]가 저장\n",
    "    category_number_dic = {'IT과학': 0, '경제': 1, '정치': 2,'세계':3,'오피니언':4, '사회': 5, '생활문화': 6}\n",
    "    all_temp.append(category_number_dic.get(category)) ///all_temp에 카테고리 매핑정보까지 저장\n",
    "    token.append(all_temp)\n",
    "print(\"토큰 처리 완료\")\n",
    "    ////앞과정과 동일하게 토큰 만들기\n",
    "\n",
    "tokens = np.array(token)\n",
    "print(\"token 처리 완료\")\n",
    "print(\"train_data 최신 버전인지 확인\")\n",
    "train_X = tokens[:, 0] ///형태소들\n",
    "train_Y = tokens[:, 1] ///품사들\n",
    "\n",
    "train_Y_ = W2V.One_hot(train_Y)  ///품사들도 vec형태로\n",
    "train_X_ = W2V.Convert2Vec(\"Data\\\\post.embedding\",train_X) ///만들어둔embedingmodel 가져오기\n",
    "\n",
    "Batch_size = 32\n",
    "Total_size = len(train_X)\n",
    "Vector_size = 300\n",
    "seq_length = [len(x) for x in train_X]\n",
    "Maxseq_length = max(seq_length)\n",
    "learning_rate = 0.001\n",
    "lstm_units = 128\n",
    "num_class = 12\n",
    "training_epochs = 5\n",
    "keep_prob = 0.75\n",
    "///머신러닝 기본 요소들 설정\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, Maxseq_length, Vector_size], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, shape = [None, num_class], name = 'Y')\n",
    "seq_len = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "BiLSTM = Bi_LSTM.Bi_LSTM(lstm_units, num_class, keep_prob)\n",
    "\n",
    "with tf.variable_scope(\"loss\", reuse = tf.AUTO_REUSE): //공유 변수(W와 B가 너무 많을 때 한번에 관리해줌)\n",
    "    logits = BiLSTM.logits(X, BiLSTM.W, BiLSTM.b, seq_len) //logistic 회귀 모델을 이용한 BiLSTM\n",
    "    loss, optimizer = BiLSTM.model_build(logits, Y, learning_rate) //학습 변수로(logit,Y,leanring rate넣기)\n",
    "\n",
    "prediction = tf.nn.softmax(logits) //softmax : 결과값을 0과 1 사이로 만들어주는 함수\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1)) //correct_prediction : 예측값과 실제라벨링값이 일치하는가?\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) //일치하면1을 증가시켜서 accuracy 측정\n",
    "\n",
    "init = tf.global_variables_initializer() //변수 초기화\n",
    "\n",
    "total_batch = int(Total_size / Batch_size) //전체 batch_size 결정\n",
    "\n",
    "print(\"Start training!\")\n",
    "\n",
    "modelName = \"BiLSTM.model\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('Bidirectional_LSTM', sess.graph)\n",
    "    i = 0\n",
    "    for epoch in range(training_epochs): ///epoch 횟수만큼\n",
    "\n",
    "        avg_acc, avg_loss = 0. , 0.\n",
    "        for step in range(total_batch):\n",
    "\n",
    "            train_batch_X = train_X_[step*Batch_size : step*Batch_size+Batch_size] //+Batch_size만큼씩 늘려가면서 학습\n",
    "            train_batch_Y = train_Y_[step*Batch_size : step*Batch_size+Batch_size] //+Batch_size만큼씩 늘려가면서 학습\n",
    "            batch_seq_length = seq_length[step*Batch_size : step*Batch_size+Batch_size]\n",
    "            \n",
    "            train_batch_X = W2V.Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size) //zero_padding적용\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            //W 최적화 진행\n",
    "            loss_ = sess.run(loss, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            avg_loss += loss_ / total_batch\n",
    "            //loss 함수값 계산\n",
    "            \n",
    "            acc = sess.run(accuracy , feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            avg_acc += acc / total_batch\n",
    "            //accuracy 값 계산\n",
    "            print(\"epoch : {:02d} step : {:04d} loss = {:.6f} accuracy= {:.6f}\".format(epoch+1, step+1, loss_, acc))\n",
    "\n",
    "        summary = sess.run(BiLSTM.graph_build(avg_loss, avg_acc))       \n",
    "        train_writer.add_summary(summary, i)\n",
    "        i += 1\n",
    "\n",
    "        ///////////시간 정보 알려주는 추가 설명////////////\n",
    "    duration = time.time() - start_time\n",
    "    minute = int(duration / 60)\n",
    "    second = int(duration) % 60\n",
    "    print(\"%dminutes %dseconds\" % (minute,second))\n",
    "    save_path = saver.save(sess, os.getcwd())\n",
    "\n",
    "    train_writer.close()\n",
    "    print('save_path',save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    file = open('Article_'+category_element+'.csv','r',encoding='euc-kr',newline=\"\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\user\\\\Downloads\")\n",
    "\n",
    "category = ['IT과학','경제','정치','세계','오피니언','사회','생활문화']\n",
    "\n",
    "file_unity = open('Article_unity.csv', 'w', encoding='euc-kr')\n",
    "//열어서_저장해라(Article_unity라는 새로운 파일, 쓰기전용으로, 번역_인코딩은euc-kr유니코드한국어로)\n",
    "wcsv = csv.writer(file_unity)\n",
    "//WriteCSV에=써넣어라(file_unity를)\n",
    "count = 0\n",
    "\n",
    "for category_element in category: \n",
    "        //위에 IT과학 경제 정치 ... 를 한 요소씩 반복\n",
    "    file = open('Article_'+category_element+'.csv','r',encoding='euc-kr',newline=\"\")\n",
    "        //file에 = 열어서_저장해라(형식이 'Article_정치.csv'인 애를, 읽기전용으로,번역_인코딩은euc-kr유니코드한국어로, newline에는 빈 벡터를 두고)\n",
    "    line = csv.reader(file)\n",
    "        //line에 = 읽어들여라(file을 즉:정치 기사리스트를 한 줄씩)\n",
    "    try : \n",
    "        for line_text in line:\n",
    "                //line을 한 줄씩 반복(리스트를 반복)\n",
    "            wcsv.writerow([line_text[1], line_text[2], line_text[3], line_text[4]])\n",
    "                //아까 비어있던 file_unity를 넣은 WCSV에.한줄씩써넣어라(엑셀의[카테고리], 엑셀의[뉴스], 엑셀[본문] )\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-2e636a750cb6>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-2e636a750cb6>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    os.chdir(\"C:\\\\Users\\\\Juwon\\\\PycharmProjects\\\\tensorflows\\\\parser\\\\Csv\") // Csv가 있는 경로 설정\u001b[0m\n\u001b[1;37m                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import os\n",
    " \n",
    "os.chdir(\"C:\\\\Users\\\\Juwon\\\\PycharmProjects\\\\tensorflows\\\\parser\\\\Csv\") // Csv가 있는 경로 설정\n",
    " \n",
    "file = open('Article_unity.csv', 'r', encoding='euc-kr')\n",
    "line = file.readlines()\n",
    "random.shuffle(line)\n",
    "rcsv = csv.reader(line)\n",
    " \n",
    "file_write = open('Article_shuffled.csv', 'w', encoding='euc-kr', newline=\"\")\n",
    "wcsv = csv.writer(file_write)\n",
    " \n",
    "for i in rcsv:\n",
    "    try:\n",
    "        wcsv.writerow([i[0].strip(), i[1], i[2], i[3]])\n",
    "\t\t////아까 비어있던 file_unity를 넣은 WCSV에.한줄씩써넣어라(엑셀의 [날짜].\\n지우고, 엑셀의[카테고리], 엑셀의[뉴스], 엑셀[본문] )\n",
    "    except:\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from gensim.models import Word2Vec\n",
    "import csv //comma separated values, txt형 파일확장자\n",
    " \n",
    "twitter = Twitter()\n",
    " \n",
    "file = open(\"Article_shuffled.csv\", 'r', encoding='euc-kr')\n",
    "    //file에=열어서 저장해라(\"Article_shuffled를\")\n",
    "line = csv.reader(file)\n",
    "token = []\n",
    "embeddingmodel = []\n",
    " \n",
    "for i in line:\n",
    "    sentence = twitter.pos(i[0], norm=True, stem=True)\n",
    "        //sentence에 = twitter.내장함수인pos(part of speech=형태소)를(line안에 형태소별로 자르기)\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0])\n",
    "            ///temp_embedding에는 (형태소들 저장)\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1])\n",
    "            /// temp에는 (형태소[0] / 해당품사[1] 저장)\n",
    "        //for 문을 돌면 all_temp에는 모든 문장의 형태소[0]/품사[1]가 저장 , embeddingmodel에는 모든 문장의 품사[1]가 저장\n",
    "    all_temp.append(temp)\n",
    "        //all_temp에는 temp저장\n",
    "    embeddingmodel.append(temp_embedding)\n",
    "        //embeddingmodel에는 temp_embedding 저장\n",
    "    category_number_dic = {'IT과학': 0, '경제': 1, '정치': 2,'세계':3,'오피니언':4, '사회': 5, '생활문화': 6}\n",
    "    all_temp.append(category_number_dic.get(category))\n",
    "        ///all_temp에 카테고리 매핑정보까지 저장\n",
    "    token.append(all_temp)\n",
    "print(\"토큰 처리 완료\")\n",
    "    //여기까지 하면 all_temp에 형태소[0], 품사[1],매핑 정보 저장되어있음\n",
    " \n",
    "embeddingmodel = []\n",
    "for i in range(len(token)): ////index를 돌고\n",
    "    temp_embeddingmodel = []\n",
    "    for k in range(len(token[i][0])): ////날짜를 돌고\n",
    "        temp_embeddingmodel.append(token[i][0][k])\n",
    "    embeddingmodel.append(temp_embeddingmodel)\n",
    "        ////돌면서 temp_embeddingmodel에 넣고\n",
    "// max_vocab size 10000000 개당 1 GB 메모리 차지\n",
    "embedding = Word2Vec(embeddingmodel, size=300, window=5, min_count=10, iter=5, sg=1, max_vocab_size = 360000000)\n",
    "embedding.save('post.embedding') //만들어진 사각형 숫자 정보를 -> vector로 바꾸는 모델"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
